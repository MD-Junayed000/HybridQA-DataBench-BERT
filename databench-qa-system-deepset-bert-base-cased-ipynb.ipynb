{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10544645,"sourceType":"datasetVersion","datasetId":6524241},{"sourceId":10617391,"sourceType":"datasetVersion","datasetId":6573590}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Cleaning and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"üõ†Ô∏è **Import Required Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport ast  # For safely evaluating array-like strings\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport numpy as np\nimport re\nfrom difflib import get_close_matches\nfrom transformers import pipeline\nimport warnings\nimport torch\nfrom concurrent.futures import ProcessPoolExecutor\nimport zipfile\nimport logging\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:20:11.210547Z","iopub.execute_input":"2025-02-07T09:20:11.210777Z","iopub.status.idle":"2025-02-07T09:20:11.220445Z","shell.execute_reply.started":"2025-02-07T09:20:11.210757Z","shell.execute_reply":"2025-02-07T09:20:11.219336Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"**Cleaning Data using (Data Cleaning Function)**","metadata":{}},{"cell_type":"code","source":"def clean_data(df, folder_name):\n    \"\"\"\n    Perform data cleaning based on folder-specific requirements.\n\n    Parameters:\n    df (pd.DataFrame): The DataFrame to clean.\n    folder_name (str): The name of the folder to apply specific cleaning rules.\n\n    Returns:\n    pd.DataFrame: The cleaned DataFrame.\n    \"\"\"\n    # General cleaning logic\n    df = df.copy()\n    \n    # Apply folder-specific cleaning logic\n    if folder_name == \"080_Books\":\n        columns_fill_zero = ['Copies Left', 'Wished Users', 'Reviews']\n        df[columns_fill_zero] = df[columns_fill_zero].fillna(0)\n        numerical_columns = ['Ratings']\n        df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n        categorical_columns = ['Publication']\n        df[categorical_columns] = df[categorical_columns].fillna('Unknown')\n        text_columns = ['Book Title', 'Author', 'Category', 'Stock Status', 'Edition', 'Publication']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n        df['Price (TK)'] = pd.to_numeric(df['Price (TK)'], errors='coerce')\n\n    elif folder_name == \"079_Coffee\":\n        text_columns = ['store_location', 'product_category', 'product_type', 'product_detail', 'Month_1', 'Weekday_1']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n        df['Revenue'] = df['Revenue'].astype(str).str.replace(',', '.').str.strip()\n        numeric_columns = ['transaction_qty', 'unit_price']\n        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n\n    elif folder_name == \"078_Fires\":\n        text_columns = ['calendar_names_1', 'calendar_names_2', 'calendar_1', 'calendar_2']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n        numeric_columns = ['area', 'DMC', 'DC', 'temp', 'ISI', 'wind']\n        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n\n    elif folder_name == \"076_NBA\":\n        text_columns = ['Season_type', 'PLAYER', 'TEAM']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n    elif folder_name == \"075_Mortality\":\n        text_columns = ['Region', 'Status', 'Sex', 'Cause']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n    elif folder_name == \"074_Lift\":\n        text_columns = ['Lifter Name', 'Weight Class', 'Lift Type']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n    elif folder_name == \"071_COL\":\n        text_columns = ['Country']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n    elif folder_name == \"070_OpenFoodFacts\":\n        # Handle Missing Values\n        for col in df.columns:\n            if df[col].dtype == \"object\":\n                df[col] = df[col].fillna(\"unknown\")\n            else:\n                df[col] = df[col].fillna(0)\n\n        # Standardize Text Data\n        object_columns = df.select_dtypes(include=[\"object\"]).columns\n        for col in object_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n        # Handle Array-Like Data\n        for col in df.columns:\n            if df[col].dtype == \"object\":\n                def process_array(value):\n                    try:\n                        array = ast.literal_eval(value)\n                        if isinstance(array, list):\n                            if not array:\n                                return [\"unknown\"]\n                            return [str(item).lower().strip() if isinstance(item, str) else item for item in array]\n                        else:\n                            return value\n                    except (ValueError, SyntaxError):\n                        return value\n                df[col] = df[col].apply(process_array)\n\n        # Ensure Correct Data Types\n        for col in df.columns:\n            if df[col].dtype == \"object\":\n                try:\n                    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n                except ValueError:\n                    pass\n\n        # Process Specific Columns\n        def process_column(entry):\n            if isinstance(entry, list):\n                return [item.strip().lower() for item in entry if isinstance(item, str)]\n            elif isinstance(entry, str):\n                entry = entry.strip('[]')  # Remove the outer brackets\n                return [item.strip().lower() for item in entry.split(',') if item.strip()]\n            else:\n                return entry\n\n        columns_to_process = [\n            'categories_en', 'states_en', 'brands', 'labels_en', 'stores',\n            'countries_en', 'ingredients_analysis_tags', 'ingredients_tags'\n        ]\n\n        for column in columns_to_process:\n            if column in df.columns:\n                df[column] = df[column].apply(process_column)\n\n    elif folder_name == \"069_Taxonomy\":\n        df.drop(columns=['Unnamed: 7'], inplace=True, errors='ignore')\n        df.dropna(subset=['Unique ID', 'Parent'], inplace=True)\n        for column in ['Tier 2', 'Tier 3', 'Tier 4']:\n            if column in df.columns and pd.api.types.is_categorical_dtype(df[column]):\n                df[column] = df[column].cat.add_categories('Unknown')\n                df[column] = df[column].fillna('Unknown')\n\n        text_columns = ['Name', 'Tier 1', 'Tier 2', 'Tier 3', 'Tier 4']\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n        df.drop_duplicates(inplace=True)\n\n    elif folder_name == \"068_WorldBank_Awards\":\n        # Ensure the 'Procurement Method' column is handled as a categorical column\n        if 'Procurement Method' in df.columns:\n            if pd.api.types.is_categorical_dtype(df['Procurement Method']):\n                 # Add 'Unknown' category if not already present\n                 if 'Unknown' not in df['Procurement Method'].cat.categories:\n                      df['Procurement Method'] = df['Procurement Method'].cat.add_categories('Unknown')\n            df['Procurement Method'] = df['Procurement Method'].fillna('Unknown')\n\n        # Ensure 'Project Global Practice' column is handled as a categorical column\n        if 'Project Global Practice' in df.columns:\n            if pd.api.types.is_categorical_dtype(df['Project Global Practice']):\n                # Add 'Not Specified' category if not already present\n                if 'Not Specified' not in df['Project Global Practice'].cat.categories:\n                     df['Project Global Practice'] = df['Project Global Practice'].cat.add_categories('Not Specified')\n            df['Project Global Practice'] = df['Project Global Practice'].fillna('Not Specified')\n\n        \n        if 'Borrower Contract Reference Number' in df.columns:\n            df['Borrower Contract Reference Number'] = df['Borrower Contract Reference Number'].cat.add_categories('N/A')\n            df['Borrower Contract Reference Number'] = df['Borrower Contract Reference Number'].fillna('N/A')\n\n        df['Supplier ID'] = df['Supplier ID'].fillna(-1)\n        df['Contract Description'] = df['Contract Description'].fillna('No Description Provided')\n        if 'Supplier Country Code' in df.columns:\n            df['Supplier Country Code'] = df['Supplier Country Code'].cat.add_categories('Unknown')\n            df['Supplier Country Code'] = df['Supplier Country Code'].fillna('Unknown')\n\n        if 'Supplier Country' in df.columns and pd.api.types.is_categorical_dtype(df['Supplier Country']):\n             df['Supplier Country'] = df['Supplier Country'].cat.add_categories('Unknown')\n             df['Supplier Country'] = df['Supplier Country'].fillna('Unknown')\n\n        if 'Supplier' in df.columns and pd.api.types.is_categorical_dtype(df['Supplier']):\n             df['Supplier'] = df['Supplier'].cat.add_categories('Unknown Supplier')\n             df['Supplier'] = df['Supplier'].fillna('Unknown Supplier')\n\n        df.drop(columns=['Borrower Country Code'], inplace=True, errors='ignore')\n        text_columns = [\n            'Procurement Method', 'Project Global Practice', 'Contract Description',\n            'Supplier Country Code', 'Borrower Country', 'Region',\n            'Supplier Country', 'Supplier', 'Project Name'\n        ]\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n        df['Region'] = df['Region'].str.replace('_', ' ')\n        df.drop_duplicates(inplace=True)\n\n    elif folder_name == \"067_TripAdvisor\":\n        def process_text(text):\n            lemmatizer = WordNetLemmatizer()\n            #stemmer = PorterStemmer() ####### not utilizing it\n            text = text.lower().strip()\n            tokens = word_tokenize(text)\n            return \" \".join([\n                lemmatizer.lemmatize(token)  # Only lemmatize the tokens\n                #stemmer.stem(lemmatizer.lemmatize(token))\n                for token in tokens if token not in ENGLISH_STOP_WORDS\n            ])\n        \n        # Ensure 'date_stayed' is processed correctly\n        if 'date_stayed' in df.columns:\n            if pd.api.types.is_categorical_dtype(df['date_stayed']):\n                df['date_stayed'] = df['date_stayed'].cat.add_categories(['Unknown'])\n            df['date_stayed'] = df['date_stayed'].fillna('Unknown')\n        \n        # Process other text columns\n        if 'title' in df.columns:\n            df['title'] = df['title'].astype(str).fillna(\"\").str.lower().str.strip()\n\n        if 'ratings' in df.columns:\n            df['ratings'] = df['ratings'].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n        if 'text' in df.columns:\n            df['text'] = df['text'].apply(process_text)\n\n    elif folder_name == \"066_IBM_HR\":\n        text_columns = df.select_dtypes(include=['object']).columns\n        for col in text_columns:\n            df[col] = df[col].str.lower().str.strip()\n\n\n\n    return df\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Processing Parquet Files***\n(Conversion to CSV)","metadata":{}},{"cell_type":"code","source":"def process_parquet_files(base_folder):\n    \"\"\"\n    Process parquet files for each folder: read, clean, and save as CSV.\n\n    Parameters:\n    base_folder (str): Path to the competition folder.\n\n    Returns:\n    None\n    \"\"\"\n    for folder_name in os.listdir(base_folder):\n        folder_path = os.path.join(base_folder, folder_name)\n        \n        if os.path.isdir(folder_path):\n            print(f\"Processing folder: {folder_name}\")\n            \n            # Process 'all.parquet'\n            all_parquet_path = os.path.join(folder_path, \"all.parquet\")\n            all_csv_path = os.path.join(folder_path, \"cleaned_all.csv\")\n            if os.path.exists(all_parquet_path):\n                all_df = pd.read_parquet(all_parquet_path)\n                cleaned_all_df = clean_data(all_df, folder_name)\n                cleaned_all_df.to_csv(all_csv_path, index=False)\n                print(f\"Processed and saved: {all_csv_path}\")\n            \n            # Process 'sample.parquet'\n            sample_parquet_path = os.path.join(folder_path, \"sample.parquet\")\n            sample_csv_path = os.path.join(folder_path, \"cleaned_sample.csv\")\n            if os.path.exists(sample_parquet_path):\n                sample_df = pd.read_parquet(sample_parquet_path)\n                cleaned_sample_df = clean_data(sample_df, folder_name)\n                cleaned_sample_df.to_csv(sample_csv_path, index=False)\n                print(f\"Processed and saved: {sample_csv_path}\")\n\n# Usage\nbase_folder = r\"C:\\Users\\ASUS\\Downloads\\competition\\competition\" # Replace with your actual path\nprocess_parquet_files(base_folder)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced Question Answering Extraction System","metadata":{}},{"cell_type":"code","source":"pip install transformers sentence-transformers pandas numpy torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:20:11.225429Z","iopub.execute_input":"2025-02-07T09:20:11.225786Z","iopub.status.idle":"2025-02-07T09:20:15.611718Z","shell.execute_reply.started":"2025-02-07T09:20:11.225751Z","shell.execute_reply":"2025-02-07T09:20:15.610567Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"***Setting Up QA Pipeline***\n\n(Model Usage in Answer Extraction)","metadata":{}},{"cell_type":"code","source":"# ========== CONFIGURATION ==========\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nDEVICE = 0 if torch.cuda.is_available() else -1\nMODEL_NAME = \"deepset/bert-base-cased-squad2\"\nDATA_DIR = \"/kaggle/input/competition-csv/competition_csv/competition\"\nQA_FILE = os.path.join(DATA_DIR, \"test_qa.csv\")\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\nprint(\"‚úÖ Loading BERT-based Question Answering pipeline...\")\nqa_pipeline = pipeline(\"question-answering\", model=MODEL_NAME, device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:21:53.290044Z","iopub.execute_input":"2025-02-07T09:21:53.290390Z","iopub.status.idle":"2025-02-07T09:21:53.692798Z","shell.execute_reply.started":"2025-02-07T09:21:53.290365Z","shell.execute_reply":"2025-02-07T09:21:53.692034Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Loading BERT-based Question Answering pipeline...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ========== TEXT UTILITY FUNCTIONS ==========\ndef normalize_text(text):\n    \"\"\"Normalize text for matching operations\"\"\"\n    return re.sub(r'[^\\w\\s]', '', str(text).lower())\n\ndef find_best_column(question, columns, df):\n    \"\"\"Find the best matching column using multiple strategies\"\"\"\n    question = normalize_text(question)\n    columns_lower = [normalize_text(col) for col in columns]\n\n    # Try exact match first\n    for col, col_lower in zip(columns, columns_lower):\n        if col_lower in question:\n            return col if col in df.columns else None\n\n    # Use difflib for approximate matching\n    matches = get_close_matches(question, columns_lower, n=1, cutoff=0.7)\n    return columns[columns_lower.index(matches[0])] if matches else None\n\ndef extract_number(question):\n    \"\"\"Extract numbers from question text. Return 0 if no number is found.\"\"\"\n    numbers = re.findall(r'\\d+\\.?\\d*', question)\n    return float(numbers[0]) if numbers else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:21:54.023573Z","iopub.execute_input":"2025-02-07T09:21:54.023924Z","iopub.status.idle":"2025-02-07T09:21:54.030453Z","shell.execute_reply.started":"2025-02-07T09:21:54.023894Z","shell.execute_reply":"2025-02-07T09:21:54.029378Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"***Defining a Class to Process Questions***\n\nRule-Based Question Handling","metadata":{}},{"cell_type":"code","source":"# ========== QUESTION HANDLER ==========\nclass QuestionHandler:\n    def __init__(self, df):\n        # Convert columns to numeric where possible\n        self.df = df.copy()\n        for col in self.df.columns:\n            self.df[col] = pd.to_numeric(self.df[col], errors='ignore')\n        self.original_columns = self.df.columns.tolist()\n        self.numeric_columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n\n    def handle(self, question):\n        \"\"\"Main entry point for handling questions\"\"\"\n        question = normalize_text(question)\n        handlers = [self.handle_boolean, self.handle_category, self.handle_number, self.handle_list]\n\n        for handler in handlers:\n            result = handler(question)\n            if result is not None:\n                return result\n\n        return \"Unhandled question type\"\n\n    #Processing Boolean Questions\n\n    def handle_boolean(self, question):\n        \"\"\"Handle Yes/No questions\"\"\"\n        if re.search(r'\\b(is|are|does|do|was|were|has|have|had|can|could|should|will|would)\\b', question, re.IGNORECASE):\n            column = find_best_column(question, self.original_columns, self.df)\n            if column and column in self.numeric_columns:\n                value = extract_number(question)\n                if re.search(r'\\b(greater|more than|above)\\b', question):\n                    return \"Yes\" if (self.df[column] > value).any() else \"No\"\n                elif re.search(r'\\b(less than|below)\\b', question):\n                    return \"Yes\" if (self.df[column] < value).any() else \"No\"\n            return \"Yes\" if any(normalize_text(str(val)) in question for val in self.df.values.flatten()) else \"No\"\n        return None\n\n    def handle_category(self, question):\n        \"\"\"Handle questions expecting a single category value\"\"\"\n        if re.search(r'\\b(which|what|who)\\b', question):\n            column = find_best_column(question, self.original_columns, self.df)\n            if column:\n                if column in self.numeric_columns:\n                    return str(self.df[column].mode()[0])\n                else:\n                    return str(self.df[column].mode()[0])\n        return None\n\n    def handle_number(self, question):\n        \"\"\"Handle numerical questions\"\"\"\n        if re.search(r'\\b(how many|number of|count|sum|total|average|mean|maximum|minimum|median)\\b', question):\n            column = find_best_column(question, self.original_columns, self.df)\n            if column and column in self.numeric_columns:\n                if re.search(r'\\b(sum|total)\\b', question):\n                    return str(int(self.df[column].sum()))\n                elif re.search(r'\\b(average|mean)\\b', question):\n                    return f\"{self.df[column].mean():.2f}\"\n                elif re.search(r'\\b(maximum|max)\\b', question):\n                    return str(int(self.df[column].max()))\n                elif re.search(r'\\b(minimum|min)\\b', question):\n                    return str(int(self.df[column].min()))\n                elif re.search(r'\\b(median)\\b', question):\n                    return f\"{self.df[column].median():.2f}\"\n                else:\n                    return str(int(self.df[column].count()))\n        return None\n\n    def handle_list(self, question):\n        \"\"\"Handle list-type questions\"\"\"\n        if re.search(r'\\b(list|unique|top|most common)\\b', question):\n            column = find_best_column(question, self.original_columns, self.df)\n            if column:\n                if re.search(r'\\b(top|most common)\\b', question):\n                    values = self.df[column].value_counts().index.tolist()[:5]\n                else:\n                    values = self.df[column].dropna().unique().tolist()[:5]\n\n                if column in self.numeric_columns:\n                    values = [float(v) if isinstance(v, (int, float)) else v for v in values]\n                    return str(values)\n                else:\n                    return str(values)\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:21:54.571620Z","iopub.execute_input":"2025-02-07T09:21:54.571950Z","iopub.status.idle":"2025-02-07T09:21:54.584355Z","shell.execute_reply.started":"2025-02-07T09:21:54.571923Z","shell.execute_reply":"2025-02-07T09:21:54.583526Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"***Processing and Saving the Predictions***\n\nExtracting answers using rule-based and BERT model-based methods.","metadata":{}},{"cell_type":"code","source":"# ========== MAIN PROCESSING ==========\ndef process_qa_file(output_filename, file_type, max_rows=None):\n    \"\"\"\n    Process QA predictions with enhanced answer formatting\n    \"\"\"\n    qa_df = pd.read_csv(QA_FILE)\n    \n    if max_rows:\n        qa_df = qa_df.head(max_rows)\n\n    predictions = []\n\n    for _, row in qa_df.iterrows():\n        dataset_name = row['dataset']\n        question = row['question']\n\n        # Load the dataset\n        dataset_path = os.path.join(DATA_DIR, dataset_name, f\"{file_type}.csv\")\n        if not os.path.exists(dataset_path):\n            predictions.append(\"Dataset not found\")\n            continue\n\n        df = pd.read_csv(dataset_path, nrows=500)\n        handler = QuestionHandler(df)\n\n        # Handle rule-based processing first\n        rule_based_answer = handler.handle(question)\n\n        if rule_based_answer and \"Unhandled\" not in rule_based_answer:\n            predictions.append(str(rule_based_answer))\n        else:\n            # Fallback to QA model with type-aware processing\n            # Running the Model for Answer Extraction\n            try:\n                context = df.head(50).to_string(index=False)\n                model_answer = qa_pipeline({\"question\": question, \"context\": context})\n                answer_text = model_answer['answer']\n                \n                # Boolean answer post-processing\n                if re.search(r'^\\s*(Is|Does|Do|Are|Was|Were|Has|Have|Had|Can|Could|Should|Will|Would)', \n                            question, re.IGNORECASE):\n                    if answer_text.lower() in ['yes', 'no']:\n                        final_answer = answer_text.capitalize()\n                    else:\n                        exists = any(answer_text.lower() in str(cell).lower() \n                                   for cell in df.values.flatten())\n                        final_answer = \"Yes\" if exists else \"No\"\n                else:\n                    final_answer = answer_text\n                \n                predictions.append(final_answer)\n            except Exception as e:\n                logging.error(f\"Error processing question: {question}. Error: {str(e)}\")\n                predictions.append(f\"Error: {str(e)}\")\n\n    # Save predictions with proper formatting\n    output_file = f\"{output_filename}.txt\"\n    with open(output_file, \"w\") as f:\n        f.write(\"\\n\".join(predictions))\n\n    print(f\"‚úÖ Predictions saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    # Process full dataset\n    process_qa_file(\"predictions\", \"cleaned_all\")\n    \n    # Process lite version\n    process_qa_file(\"predictions_lite\", \"cleaned_sample\", max_rows=20)\n    \n    # Create archive\n    with zipfile.ZipFile(\"hope_little_rockstar.zip\", \"w\") as zipf:\n        zipf.write(\"predictions.txt\")\n        zipf.write(\"predictions_lite.txt\")\n\n    print(\"‚úÖ Archive.zip created successfully\")","metadata":{"execution":{"iopub.status.busy":"2025-02-07T09:21:55.094929Z","iopub.execute_input":"2025-02-07T09:21:55.095261Z","iopub.status.idle":"2025-02-07T09:22:25.369869Z","shell.execute_reply.started":"2025-02-07T09:21:55.095234Z","shell.execute_reply":"2025-02-07T09:22:25.369107Z"},"trusted":true},"outputs":[{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Predictions saved to predictions.txt\n‚úÖ Predictions saved to predictions_lite.txt\n‚úÖ Archive.zip created successfully\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}