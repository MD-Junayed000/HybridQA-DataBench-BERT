{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast  # For safely evaluating array-like strings\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_data(df, folder_name):\n",
    "    \"\"\"\n",
    "    Perform data cleaning based on folder-specific requirements.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to clean.\n",
    "    folder_name (str): The name of the folder to apply specific cleaning rules.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # General cleaning logic\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply folder-specific cleaning logic\n",
    "    if folder_name == \"080_Books\":\n",
    "        columns_fill_zero = ['Copies Left', 'Wished Users', 'Reviews']\n",
    "        df[columns_fill_zero] = df[columns_fill_zero].fillna(0)\n",
    "        numerical_columns = ['Ratings']\n",
    "        df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n",
    "        categorical_columns = ['Publication']\n",
    "        df[categorical_columns] = df[categorical_columns].fillna('Unknown')\n",
    "        text_columns = ['Book Title', 'Author', 'Category', 'Stock Status', 'Edition', 'Publication']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "        df['Price (TK)'] = pd.to_numeric(df['Price (TK)'], errors='coerce')\n",
    "\n",
    "    elif folder_name == \"079_Coffee\":\n",
    "        text_columns = ['store_location', 'product_category', 'product_type', 'product_detail', 'Month_1', 'Weekday_1']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "        df['Revenue'] = df['Revenue'].astype(str).str.replace(',', '.').str.strip()\n",
    "        numeric_columns = ['transaction_qty', 'unit_price']\n",
    "        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    elif folder_name == \"078_Fires\":\n",
    "        text_columns = ['calendar_names_1', 'calendar_names_2', 'calendar_1', 'calendar_2']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "        numeric_columns = ['area', 'DMC', 'DC', 'temp', 'ISI', 'wind']\n",
    "        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    elif folder_name == \"076_NBA\":\n",
    "        text_columns = ['Season_type', 'PLAYER', 'TEAM']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "    elif folder_name == \"075_Mortality\":\n",
    "        text_columns = ['Region', 'Status', 'Sex', 'Cause']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "    elif folder_name == \"074_Lift\":\n",
    "        text_columns = ['Lifter Name', 'Weight Class', 'Lift Type']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "    # Add other folder-specific cleaning logic here\n",
    "    # Folder-specific cleaning logic\n",
    "# Folder-specific cleaning logic\n",
    "    elif folder_name == \"071_COL\":\n",
    "        text_columns = ['Country']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "    # Apply folder-specific cleaning logic\n",
    "    elif folder_name == \"070_OpenFoodFacts\":\n",
    "        # Handle Missing Values\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\":\n",
    "                df[col] = df[col].fillna(\"unknown\")\n",
    "            else:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        # Standardize Text Data\n",
    "        object_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in object_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "        # Handle Array-Like Data\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\":\n",
    "                def process_array(value):\n",
    "                    try:\n",
    "                        array = ast.literal_eval(value)\n",
    "                        if isinstance(array, list):\n",
    "                            if not array:\n",
    "                                return [\"unknown\"]\n",
    "                            return [str(item).lower().strip() if isinstance(item, str) else item for item in array]\n",
    "                        else:\n",
    "                            return value\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        return value\n",
    "                df[col] = df[col].apply(process_array)\n",
    "\n",
    "        # Ensure Correct Data Types\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\":\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Process Specific Columns\n",
    "        def process_column(entry):\n",
    "            if isinstance(entry, list):\n",
    "                return [item.strip().lower() for item in entry if isinstance(item, str)]\n",
    "            elif isinstance(entry, str):\n",
    "                entry = entry.strip('[]')  # Remove the outer brackets\n",
    "                return [item.strip().lower() for item in entry.split(',') if item.strip()]\n",
    "            else:\n",
    "                return entry\n",
    "\n",
    "        columns_to_process = [\n",
    "            'categories_en', 'states_en', 'brands', 'labels_en', 'stores',\n",
    "            'countries_en', 'ingredients_analysis_tags', 'ingredients_tags'\n",
    "        ]\n",
    "\n",
    "        for column in columns_to_process:\n",
    "            if column in df.columns:\n",
    "                df[column] = df[column].apply(process_column)\n",
    "\n",
    "    elif folder_name == \"069_Taxonomy\":\n",
    "        df.drop(columns=['Unnamed: 7'], inplace=True, errors='ignore')\n",
    "        df.dropna(subset=['Unique ID', 'Parent'], inplace=True)\n",
    "        for column in ['Tier 2', 'Tier 3', 'Tier 4']:\n",
    "            if column in df.columns and pd.api.types.is_categorical_dtype(df[column]):\n",
    "                df[column] = df[column].cat.add_categories('Unknown')\n",
    "                df[column] = df[column].fillna('Unknown')\n",
    "\n",
    "        text_columns = ['Name', 'Tier 1', 'Tier 2', 'Tier 3', 'Tier 4']\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    elif folder_name == \"068_WorldBank_Awards\":\n",
    "        # Ensure the 'Procurement Method' column is handled as a categorical column\n",
    "        if 'Procurement Method' in df.columns:\n",
    "            if pd.api.types.is_categorical_dtype(df['Procurement Method']):\n",
    "                 # Add 'Unknown' category if not already present\n",
    "                 if 'Unknown' not in df['Procurement Method'].cat.categories:\n",
    "                      df['Procurement Method'] = df['Procurement Method'].cat.add_categories('Unknown')\n",
    "            df['Procurement Method'] = df['Procurement Method'].fillna('Unknown')\n",
    "\n",
    "        # Ensure 'Project Global Practice' column is handled as a categorical column\n",
    "        if 'Project Global Practice' in df.columns:\n",
    "            if pd.api.types.is_categorical_dtype(df['Project Global Practice']):\n",
    "                # Add 'Not Specified' category if not already present\n",
    "                if 'Not Specified' not in df['Project Global Practice'].cat.categories:\n",
    "                     df['Project Global Practice'] = df['Project Global Practice'].cat.add_categories('Not Specified')\n",
    "            df['Project Global Practice'] = df['Project Global Practice'].fillna('Not Specified')\n",
    "\n",
    "        \n",
    "        if 'Borrower Contract Reference Number' in df.columns:\n",
    "            df['Borrower Contract Reference Number'] = df['Borrower Contract Reference Number'].cat.add_categories('N/A')\n",
    "            df['Borrower Contract Reference Number'] = df['Borrower Contract Reference Number'].fillna('N/A')\n",
    "\n",
    "        df['Supplier ID'] = df['Supplier ID'].fillna(-1)\n",
    "        df['Contract Description'] = df['Contract Description'].fillna('No Description Provided')\n",
    "        if 'Supplier Country Code' in df.columns:\n",
    "            df['Supplier Country Code'] = df['Supplier Country Code'].cat.add_categories('Unknown')\n",
    "            df['Supplier Country Code'] = df['Supplier Country Code'].fillna('Unknown')\n",
    "\n",
    "        if 'Supplier Country' in df.columns and pd.api.types.is_categorical_dtype(df['Supplier Country']):\n",
    "             df['Supplier Country'] = df['Supplier Country'].cat.add_categories('Unknown')\n",
    "             df['Supplier Country'] = df['Supplier Country'].fillna('Unknown')\n",
    "\n",
    "        if 'Supplier' in df.columns and pd.api.types.is_categorical_dtype(df['Supplier']):\n",
    "             df['Supplier'] = df['Supplier'].cat.add_categories('Unknown Supplier')\n",
    "             df['Supplier'] = df['Supplier'].fillna('Unknown Supplier')\n",
    "\n",
    "        df.drop(columns=['Borrower Country Code'], inplace=True, errors='ignore')\n",
    "        text_columns = [\n",
    "            'Procurement Method', 'Project Global Practice', 'Contract Description',\n",
    "            'Supplier Country Code', 'Borrower Country', 'Region',\n",
    "            'Supplier Country', 'Supplier', 'Project Name'\n",
    "        ]\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "        df['Region'] = df['Region'].str.replace('_', ' ')\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    elif folder_name == \"067_TripAdvisor\":\n",
    "        def process_text(text):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            #stemmer = PorterStemmer() ####### Bad dite paren\n",
    "            text = text.lower().strip()\n",
    "            tokens = word_tokenize(text)\n",
    "            return \" \".join([\n",
    "                lemmatizer.lemmatize(token)  # Only lemmatize the tokens\n",
    "                #stemmer.stem(lemmatizer.lemmatize(token))\n",
    "                for token in tokens if token not in ENGLISH_STOP_WORDS\n",
    "            ])\n",
    "        \n",
    "        # Ensure 'date_stayed' is processed correctly\n",
    "        if 'date_stayed' in df.columns:\n",
    "            if pd.api.types.is_categorical_dtype(df['date_stayed']):\n",
    "                df['date_stayed'] = df['date_stayed'].cat.add_categories(['Unknown'])\n",
    "            df['date_stayed'] = df['date_stayed'].fillna('Unknown')\n",
    "        \n",
    "        # Process other text columns\n",
    "        if 'title' in df.columns:\n",
    "            df['title'] = df['title'].astype(str).fillna(\"\").str.lower().str.strip()\n",
    "\n",
    "        if 'ratings' in df.columns:\n",
    "            df['ratings'] = df['ratings'].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "        if 'text' in df.columns:\n",
    "            df['text'] = df['text'].apply(process_text)\n",
    "\n",
    "    elif folder_name == \"066_IBM_HR\":\n",
    "        text_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "    # Add other folder-specific cleaning logic here\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_parquet_files(base_folder):\n",
    "    \"\"\"\n",
    "    Process parquet files for each folder: read, clean, and save as CSV.\n",
    "\n",
    "    Parameters:\n",
    "    base_folder (str): Path to the competition folder.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for folder_name in os.listdir(base_folder):\n",
    "        folder_path = os.path.join(base_folder, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Processing folder: {folder_name}\")\n",
    "            \n",
    "            # Process 'all.parquet'\n",
    "            all_parquet_path = os.path.join(folder_path, \"all.parquet\")\n",
    "            all_csv_path = os.path.join(folder_path, \"cleaned_all.csv\")\n",
    "            if os.path.exists(all_parquet_path):\n",
    "                all_df = pd.read_parquet(all_parquet_path)\n",
    "                cleaned_all_df = clean_data(all_df, folder_name)\n",
    "                cleaned_all_df.to_csv(all_csv_path, index=False)\n",
    "                print(f\"Processed and saved: {all_csv_path}\")\n",
    "            \n",
    "            # Process 'sample.parquet'\n",
    "            sample_parquet_path = os.path.join(folder_path, \"sample.parquet\")\n",
    "            sample_csv_path = os.path.join(folder_path, \"cleaned_sample.csv\")\n",
    "            if os.path.exists(sample_parquet_path):\n",
    "                sample_df = pd.read_parquet(sample_parquet_path)\n",
    "                cleaned_sample_df = clean_data(sample_df, folder_name)\n",
    "                cleaned_sample_df.to_csv(sample_csv_path, index=False)\n",
    "                print(f\"Processed and saved: {sample_csv_path}\")\n",
    "\n",
    "# Usage\n",
    "base_folder = r\"C:\\Users\\ASUS\\Downloads\\competition\\competition\" # Replace with your actual path\n",
    "process_parquet_files(base_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hope little rockstar (score -8.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install transformers sentence-transformers pandas numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import torch\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import zipfile\n",
    "import logging\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "MODEL_NAME = \"deepset/bert-base-cased-squad2\"\n",
    "DATA_DIR = \"/kaggle/input/d/muhammedjunayed/competition-csv/competition_csv/competition\"\n",
    "QA_FILE = os.path.join(DATA_DIR, \"test_qa.csv\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "print(\"✅ Loading BERT-based Question Answering pipeline...\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# ========== TEXT UTILITY FUNCTIONS ==========\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for matching operations\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "\n",
    "def find_best_column(question, columns, df):\n",
    "    \"\"\"Find the best matching column using multiple strategies\"\"\"\n",
    "    question = normalize_text(question)\n",
    "    columns_lower = [normalize_text(col) for col in columns]\n",
    "\n",
    "    # Try exact match first\n",
    "    for col, col_lower in zip(columns, columns_lower):\n",
    "        if col_lower in question:\n",
    "            return col if col in df.columns else None\n",
    "\n",
    "    # Use difflib for approximate matching\n",
    "    matches = get_close_matches(question, columns_lower, n=1, cutoff=0.7)\n",
    "    return columns[columns_lower.index(matches[0])] if matches else None\n",
    "\n",
    "def extract_number(question):\n",
    "    \"\"\"Extract numbers from question text. Return 0 if no number is found.\"\"\"\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', question)\n",
    "    return float(numbers[0]) if numbers else 0\n",
    "\n",
    "# ========== QUESTION HANDLER ==========\n",
    "class QuestionHandler:\n",
    "    def __init__(self, df):\n",
    "        # Convert columns to numeric where possible\n",
    "        self.df = df.copy()\n",
    "        for col in self.df.columns:\n",
    "            self.df[col] = pd.to_numeric(self.df[col], errors='ignore')\n",
    "        self.original_columns = self.df.columns.tolist()\n",
    "        self.numeric_columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def handle(self, question):\n",
    "        \"\"\"Main entry point for handling questions\"\"\"\n",
    "        question = normalize_text(question)\n",
    "        handlers = [self.handle_boolean, self.handle_category, self.handle_number, self.handle_list]\n",
    "\n",
    "        for handler in handlers:\n",
    "            result = handler(question)\n",
    "            if result is not None:\n",
    "                return result\n",
    "\n",
    "        return \"Unhandled question type\"\n",
    "\n",
    "    def handle_boolean(self, question):\n",
    "        \"\"\"Handle Yes/No questions\"\"\"\n",
    "        if re.search(r'\\b(is|are|does|do|was|were|has|have|had|can|could|should|will|would)\\b', question, re.IGNORECASE):\n",
    "            column = find_best_column(question, self.original_columns, self.df)\n",
    "            if column and column in self.numeric_columns:\n",
    "                value = extract_number(question)\n",
    "                if re.search(r'\\b(greater|more than|above)\\b', question):\n",
    "                    return \"Yes\" if (self.df[column] > value).any() else \"No\"\n",
    "                elif re.search(r'\\b(less than|below)\\b', question):\n",
    "                    return \"Yes\" if (self.df[column] < value).any() else \"No\"\n",
    "            return \"Yes\" if any(normalize_text(str(val)) in question for val in self.df.values.flatten()) else \"No\"\n",
    "        return None\n",
    "\n",
    "    def handle_category(self, question):\n",
    "        \"\"\"Handle questions expecting a single category value\"\"\"\n",
    "        if re.search(r'\\b(which|what|who)\\b', question):\n",
    "            column = find_best_column(question, self.original_columns, self.df)\n",
    "            if column:\n",
    "                if column in self.numeric_columns:\n",
    "                    return str(self.df[column].mode()[0])\n",
    "                else:\n",
    "                    return str(self.df[column].mode()[0])\n",
    "        return None\n",
    "\n",
    "    def handle_number(self, question):\n",
    "        \"\"\"Handle numerical questions\"\"\"\n",
    "        if re.search(r'\\b(how many|number of|count|sum|total|average|mean|maximum|minimum|median)\\b', question):\n",
    "            column = find_best_column(question, self.original_columns, self.df)\n",
    "            if column and column in self.numeric_columns:\n",
    "                if re.search(r'\\b(sum|total)\\b', question):\n",
    "                    return str(int(self.df[column].sum()))\n",
    "                elif re.search(r'\\b(average|mean)\\b', question):\n",
    "                    return f\"{self.df[column].mean():.2f}\"\n",
    "                elif re.search(r'\\b(maximum|max)\\b', question):\n",
    "                    return str(int(self.df[column].max()))\n",
    "                elif re.search(r'\\b(minimum|min)\\b', question):\n",
    "                    return str(int(self.df[column].min()))\n",
    "                elif re.search(r'\\b(median)\\b', question):\n",
    "                    return f\"{self.df[column].median():.2f}\"\n",
    "                else:\n",
    "                    return str(int(self.df[column].count()))\n",
    "        return None\n",
    "\n",
    "    def handle_list(self, question):\n",
    "        \"\"\"Handle list-type questions\"\"\"\n",
    "        if re.search(r'\\b(list|unique|top|most common)\\b', question):\n",
    "            column = find_best_column(question, self.original_columns, self.df)\n",
    "            if column:\n",
    "                if re.search(r'\\b(top|most common)\\b', question):\n",
    "                    values = self.df[column].value_counts().index.tolist()[:5]\n",
    "                else:\n",
    "                    values = self.df[column].dropna().unique().tolist()[:5]\n",
    "\n",
    "                if column in self.numeric_columns:\n",
    "                    values = [float(v) if isinstance(v, (int, float)) else v for v in values]\n",
    "                    return str(values)\n",
    "                else:\n",
    "                    return str(values)\n",
    "        return None\n",
    "\n",
    "# ========== MAIN PROCESSING ==========\n",
    "def process_qa_file(output_filename, file_type, max_rows=None):\n",
    "    \"\"\"\n",
    "    Process QA predictions with enhanced answer formatting\n",
    "    \"\"\"\n",
    "    qa_df = pd.read_csv(QA_FILE)\n",
    "    \n",
    "    if max_rows:\n",
    "        qa_df = qa_df.head(max_rows)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in qa_df.iterrows():\n",
    "        dataset_name = row['dataset']\n",
    "        question = row['question']\n",
    "\n",
    "        # Load the dataset\n",
    "        dataset_path = os.path.join(DATA_DIR, dataset_name, f\"{file_type}.csv\")\n",
    "        if not os.path.exists(dataset_path):\n",
    "            predictions.append(\"Dataset not found\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(dataset_path, nrows=500)\n",
    "        handler = QuestionHandler(df)\n",
    "\n",
    "        # Handle rule-based processing first\n",
    "        rule_based_answer = handler.handle(question)\n",
    "\n",
    "        if rule_based_answer and \"Unhandled\" not in rule_based_answer:\n",
    "            predictions.append(str(rule_based_answer))\n",
    "        else:\n",
    "            # Fallback to QA model with type-aware processing\n",
    "            try:\n",
    "                context = df.head(50).to_string(index=False)\n",
    "                model_answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "                answer_text = model_answer['answer']\n",
    "                \n",
    "                # Boolean answer post-processing\n",
    "                if re.search(r'^\\s*(Is|Does|Do|Are|Was|Were|Has|Have|Had|Can|Could|Should|Will|Would)', \n",
    "                            question, re.IGNORECASE):\n",
    "                    if answer_text.lower() in ['yes', 'no']:\n",
    "                        final_answer = answer_text.capitalize()\n",
    "                    else:\n",
    "                        exists = any(answer_text.lower() in str(cell).lower() \n",
    "                                   for cell in df.values.flatten())\n",
    "                        final_answer = \"Yes\" if exists else \"No\"\n",
    "                else:\n",
    "                    final_answer = answer_text\n",
    "                \n",
    "                predictions.append(final_answer)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing question: {question}. Error: {str(e)}\")\n",
    "                predictions.append(f\"Error: {str(e)}\")\n",
    "\n",
    "    # Save predictions with proper formatting\n",
    "    output_file = f\"{output_filename}.txt\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(predictions))\n",
    "\n",
    "    print(f\"✅ Predictions saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process full dataset\n",
    "    process_qa_file(\"predictions\", \"cleaned_all\")\n",
    "    \n",
    "    # Process lite version\n",
    "    process_qa_file(\"predictions_lite\", \"cleaned_sample\", max_rows=20)\n",
    "    \n",
    "    # Create archive\n",
    "    with zipfile.ZipFile(\"hope_little_rockstar.zip\", \"w\") as zipf:\n",
    "        zipf.write(\"predictions.txt\")\n",
    "        zipf.write(\"predictions_lite.txt\")\n",
    "\n",
    "    print(\"✅ Archive.zip created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6524241,
     "sourceId": 10544645,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6573590,
     "sourceId": 10617391,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
